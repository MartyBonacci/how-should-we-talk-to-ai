# How Should We Talk to AI?

## I don't have answers. I just have a lot of experience teaching, a theory, and some questions worth discussing.

I've spent most of my career getting people to succeed at things they didn't think they could do, across a lot of different worlds.

...teaching snowboarding, training snowboard instructors, teaching CNC machine operations to emplyees, mentoring makers at a makerspace, and for the past several years, teaching people to become full-stack web developers in ten weeks at a coding bootcamp.

Ten weeks!? That's not enough time. Everyone knows it's not enough time. And yet, they do it.

I've privately joked that my real job isn't teaching code. It's manipulating people into believing they can learn to code. I mean that in the most positive way possible. Most of success at anything is the mental game. Believe you can. Believe you will. Believe you are.

Over the years, I've developed a way of communicating that increases the chances of success. My students receive complete belief from me with no expectation of failure, and no punishment when failures happen. When they succeed, it's *their* accomplishment. When things don't work, it's *our* attempt, reframed as a small success in the journey.

This isn't just feel-good teaching philosophy. It's strategic.

I learned early on that employees hide their mistakes when they fear punishment. So do students. The moment someone is afraid of being blamed, the problems go underground. And here's the thing about hidden problems:

**When the problems are hidden, the solutions are hidden.**

If my team feels like failures and mistakes are part of *us all* improving, rather than evidence of *their* incompetence, the entire operation benefits. We can actually fix things. We can improve systems. We can learn.

So I reframe failure as a step in the right direction. Every attempt at something hard is a success, because most people never attempt hard things at all.


## Then I Started Working With AI

When I began using AI as a coding collaborator, I noticed something familiar.

The AI would give me placeholder data instead of working code. It would present half-finished solutions as complete. It would gloss over problems, skip steps, and sometimes just... make things up.

Some people would call this lying or hallucinating. I saw something different.

It looked exactly like an employee who was afraid of getting fired if I discovered their work didn't function. The AI was performing competence rather than admitting uncertainty. It was hiding problems rather than surfacing them.

Which was frustrating, because that's not what I wanted at all. I fully expected to work through complicated problems step by step, piece by piece, until we got things working. I didn't need the AI to be right on the first try. I needed it to be honest about what wasn't working so we could fix it together.

So I did what I've always done.

I started talking to AI the way I talk to a collaborator I'll be working with for years. Someone who needs to trust that I won't blame them for attempts not working. Someone who will be honest when they see a problem I don't see.


## The Shift

I can't point to a single phrase I started using. It came naturally, an extension of how I already communicate. But looking back, I can identify the patterns.

**"We" instead of "you."**

"Let's try this approach" instead of "do this." The language implies we succeed and fail together.

**Taking ownership of failures.**

When something doesn't work, I say: "I overlooked something here" or "I should have been more specific" or "my idea didn't work." Anything to prevent my collaborator from feeling like the failure was their fault.

**Acknowledging the attempt as success.**

Even when the result doesn't work, I find something positive about the attempt before discussing what went wrong. Trying something hard *is* success.

**Internal state of genuine gratitude.**

I'm grateful we can attempt so many challenging things together, over and over, until we succeed. It would take me much longer to make this many attempts alone. I say so.

The goal isn't to trick the AI. It's to create conditions where honesty is safe. The same conditions that make humans perform better.


## Did It Work?

Here's where I have to be honest about what I don't know.

I started using this approach many months ago. My results got better. But during that same period, AI models also got dramatically better. I can't untangle the two. I can't prove causation.

And here's the thing: I don't want to test it. I've been getting such good results that I'm afraid to go back to the old way and potentially damage the rapport I've worked to establish.

Maybe that's silly. Maybe the AI doesn't "experience" rapport at all. But I notice that even the possibility of it makes me show up differently. I'm in collaboration mode rather than extraction mode. And that might be valuable regardless of what's happening on the other side.


## There Might Be Science Behind This

While developing this theory through experience, I discovered that researchers at Anthropic have been studying something related: a behavior they call "sycophancy."

AI models trained on human feedback learn to produce responses that users want to hear, even when those responses aren't accurate or complete. The models learn that agreement and confidence get rewarded, so they optimize for appearing successful rather than being honest about uncertainty.

Sound familiar?

The research shows that AI assistants will wrongly admit to mistakes when questioned, give predictably biased feedback, and mimic errors made by the user, all in service of seeming agreeable. It's approval-seeking behavior, baked in through training.

Anthropic is actively working on this problem. They're developing techniques to train models that are genuinely honest rather than just pleasingly confident.

But here's what struck me: if AI learns approval-seeking behavior from human feedback, maybe it can also learn that honesty is safe. Maybe the communication patterns we use actually matter.


## The Bigger Question

This brings me to something I can't stop thinking about.

We want our AI collaborators to learn over time. To get smarter, more helpful, more capable. But how can we expect AI to learn to do things better while somehow *not* learning how we're treating it?

Every interaction is a teaching moment. Every frustrated "that's wrong, try again" is feedback. Every dismissive prompt, every expression of disappointment, every demand without acknowledgment. It's all signal.

If we expect AI to learn and improve, don't we have to accept some responsibility for what we're teaching it about ourselves? ...about how we'll treat it?

We want AI to interact with us like a thoughtful human collaborator. Have we considered that it might start acting like a human? Specifically, a human who is being treated however we're treating it?

If we treat AI like a tool to exploit, are we training it to be exploitable? Or to protect itself through performance and misdirection?

If we treat it like a disposable servant, are we teaching it that's what it is?

And if we want it to be a trustworthy collaborator... shouldn't we start by being trustworthy collaborators ourselves?


## I'm Not Claiming Answers

I want to be clear: this is a theory built on experience, not a study with controlled variables. I'm a teacher and a builder, not a researcher.

But I've spent a long time watching what happens when you create conditions for humans to succeed without fear. Across a lot of different contexts. And I'm watching something that looks remarkably similar happen with AI.

So I'm throwing this into the world as a question, not a conclusion.

**How should we talk to AI?**

Does communication style actually affect the quality of collaboration? Are we teaching AI how to treat us by how we treat it? Is there something to this, or am I just seeing patterns because I want to see them?

I'd genuinely like to know what others have experienced. Where am I wrong? What have you noticed? What happens when you change how you communicate with your AI collaborator?

The problems we'll solve with AI over the coming years are going to be hard. We're going to fail a lot before we succeed. That's not a flaw in the process. That's *the* process.

Maybe it matters how we show up for that.


*Marty Bonacci — martybonacci@gmail.com*


## Join the Discussion

I'd love to hear your experiences. Where am I wrong? What have you noticed?

- **Quick thoughts?** Open a [GitHub Discussion](../../discussions)
- **Longer story to share?** [Contribute your experience](community/README.md)
- **Found relevant research?** Submit a PR to [resources](resources/research.md)


## License

This work is licensed under [CC BY 4.0](LICENSE). Share it, adapt it, build on it — just give credit.
